{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "#打开词典文件，返回列表\n",
    "def open_dict(Dict = 'hahah', path=r'/'):\n",
    "    path = path + '%s.txt' % Dict\n",
    "    dictionary = open(path, 'r', encoding='utf-8') #py3\n",
    "    #dictionary = open(path, 'r')\n",
    "    dict = []\n",
    "    for word in dictionary:\n",
    "        word = word.strip('\\n')\n",
    "        dict.append(word)\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "def judgeodd(num):\n",
    "    if (num % 2) == 0:\n",
    "        return 'even'\n",
    "    else:\n",
    "        return 'odd'\n",
    "\n",
    "\n",
    "deny_word = open_dict(Dict = 'notdoc', path= r'dict/')\n",
    "posdict = open_dict(Dict = 'positive', path= r'dict/')\n",
    "negdict = open_dict(Dict = 'negative', path= r'dict/')\n",
    "\n",
    "degree_word = open_dict(Dict = 'degreedoc', path= r'dict/')\n",
    "mostdict = degree_word[degree_word.index('extreme')+1 : degree_word.index('very')]#权重4，即在情感词前乘以4\n",
    "verydict = degree_word[degree_word.index('very')+1 : degree_word.index('more')]#权重3\n",
    "moredict = degree_word[degree_word.index('more')+1 : degree_word.index('ish')]#权重2\n",
    "ishdict = degree_word[degree_word.index('ish')+1 : degree_word.index('last')]#权重0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_basics(dictl):\n",
    "#     print(type(dictl))\n",
    "#     print(len(dictl))\n",
    "#     print(dictl[0])\n",
    "#     print(dictl[1])\n",
    "# print_basics(degree_word)\n",
    "# print_basics(mostdict)\n",
    "# print_basics(verydict)\n",
    "# print_basics(moredict)\n",
    "# print_basics(ishdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score_list(dataset):\n",
    "    seg_sentence = dataset.split('。')\n",
    "\n",
    "    count1 = []\n",
    "    count2 = []\n",
    "    for sen in seg_sentence: #循环遍历每一个评论\n",
    "        segtmp = jieba.lcut(sen, cut_all=False)  #把句子进行分词，以列表的形式返回\n",
    "        i = 0 #记录扫描到的词的位置\n",
    "        a = 0 #记录情感词的位置\n",
    "        poscount = 0 #积极词的第一次分值\n",
    "        poscount2 = 0 #积极词反转后的分值\n",
    "        poscount3 = 0 #积极词的最后分值（包括叹号的分值）\n",
    "        negcount = 0\n",
    "        negcount2 = 0\n",
    "        negcount3 = 0\n",
    "        for word in segtmp:\n",
    "            if word in posdict:  # 判断词语是否是情感词\n",
    "                poscount += 1\n",
    "                c = 0\n",
    "                for w in segtmp[a:i]:  # 扫描情感词前的程度词\n",
    "                    if w in mostdict:\n",
    "                        poscount *= 4.0\n",
    "                    elif w in verydict:\n",
    "                        poscount *= 3.0\n",
    "                    elif w in moredict:\n",
    "                        poscount *= 2.0\n",
    "                    elif w in ishdict:\n",
    "                        poscount *= 0.5\n",
    "                    elif w in deny_word:\n",
    "                        c += 1\n",
    "                if judgeodd(c) == 'odd':  # 扫描情感词前的否定词数\n",
    "                    poscount *= -1.0\n",
    "                    poscount2 += poscount\n",
    "                    poscount = 0\n",
    "                    poscount3 = poscount + poscount2 + poscount3\n",
    "                    poscount2 = 0\n",
    "                else:\n",
    "                    poscount3 = poscount + poscount2 + poscount3\n",
    "                    poscount = 0\n",
    "                a = i + 1  # 情感词的位置变化\n",
    "\n",
    "            elif word in negdict:  # 消极情感的分析，与上面一致\n",
    "                negcount += 1\n",
    "                d = 0\n",
    "                for w in segtmp[a:i]:\n",
    "                    if w in mostdict:\n",
    "                        negcount *= 4.0\n",
    "                    elif w in verydict:\n",
    "                        negcount *= 3.0\n",
    "                    elif w in moredict:\n",
    "                        negcount *= 2.0\n",
    "                    elif w in ishdict:\n",
    "                        negcount *= 0.5\n",
    "                    elif w in degree_word:\n",
    "                        d += 1\n",
    "                if judgeodd(d) == 'odd':\n",
    "                    negcount *= -1.0\n",
    "                    negcount2 += negcount\n",
    "                    negcount = 0\n",
    "                    negcount3 = negcount + negcount2 + negcount3\n",
    "                    negcount2 = 0\n",
    "                else:\n",
    "                    negcount3 = negcount + negcount2 + negcount3\n",
    "                    negcount = 0\n",
    "                a = i + 1\n",
    "            elif word == '！' or word == '!':  ##判断句子是否有感叹号\n",
    "                for w2 in segtmp[::-1]:  # 扫描感叹号前的情感词，发现后权值+2，然后退出循环\n",
    "                    if w2 in posdict or negdict:\n",
    "                        poscount3 += 2\n",
    "                        negcount3 += 2\n",
    "                        break\n",
    "            i += 1 # 扫描词位置前移\n",
    "\n",
    "\n",
    "            # 以下是防止出现负数的情况\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            if poscount3 < 0 and negcount3 > 0:\n",
    "                neg_count += negcount3 - poscount3\n",
    "                pos_count = 0\n",
    "            elif negcount3 < 0 and poscount3 > 0:\n",
    "                pos_count = poscount3 - negcount3\n",
    "                neg_count = 0\n",
    "            elif poscount3 < 0 and negcount3 < 0:\n",
    "                neg_count = -poscount3\n",
    "                pos_count = -negcount3\n",
    "            else:\n",
    "                pos_count = poscount3\n",
    "                neg_count = negcount3\n",
    "\n",
    "            count1.append([pos_count, neg_count])\n",
    "        count2.append(count1)\n",
    "        count1 = []\n",
    "\n",
    "    return count2\n",
    "\n",
    "# def sentiment_score(senti_score_list):\n",
    "#     score = []\n",
    "#     for review in senti_score_list:\n",
    "#         score_array = np.array(review)\n",
    "#         Pos = np.sum(score_array[:, 0])\n",
    "#         Neg = np.sum(score_array[:, 1])\n",
    "#         AvgPos = np.mean(score_array[:, 0])\n",
    "#         AvgPos = float('%.1f'%AvgPos)\n",
    "#         AvgNeg = np.mean(score_array[:, 1])\n",
    "#         AvgNeg = float('%.1f'%AvgNeg)\n",
    "#         StdPos = np.std(score_array[:, 0])\n",
    "#         StdPos = float('%.1f'%StdPos)\n",
    "#         StdNeg = np.std(score_array[:, 1])\n",
    "#         StdNeg = float('%.1f'%StdNeg)\n",
    "#         score.append([Pos, Neg, AvgPos, AvgNeg, StdPos, StdNeg])\n",
    "#         score.append\n",
    "#     return score\n",
    "\n",
    "def sentiment_score(senti_score_list):\n",
    "    score = []\n",
    "    for review in senti_score_list:\n",
    "        score_array = np.array(review)\n",
    "        Pos = np.sum(score_array[:, 0])\n",
    "        Neg = np.sum(score_array[:, 1])\n",
    "        score.append([Pos, Neg])\n",
    "        score.append\n",
    "    return score\n",
    "\n",
    "def posneg_score(senti_score_list):\n",
    "    score = []\n",
    "    for review in senti_score_list:\n",
    "        score_array = np.array(review)\n",
    "        Pos = np.sum(score_array[:, 0])\n",
    "        Neg = np.sum(score_array[:, 1])\n",
    "        score.append([Pos, Neg])\n",
    "        score.append\n",
    "    if score[0][0] > score[0][1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = '你就是个王八蛋，混账玩意!你们的手机真不好用！非常生气，我非常郁闷！！！！'\n",
    "# data2= '我好开心啊，非常非常非常高兴！今天我得了一百分，我很兴奋开心，愉快，开心'\n",
    "# print(sentiment_score(sentiment_score_list(data)))\n",
    "# print(sentiment_score(sentiment_score_list(data2)))\n",
    "# print(posneg_score(sentiment_score_list(data2)))\n",
    "# print(sentiment_score_list(data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感评分列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个对英美一条评论和弹幕的情感得分表（0，1）\n",
    "######read danmu\n",
    "with open(\"allacfundanmu_1.txt\", encoding = \"utf-8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "danmulist = []\n",
    "danmucode = []\n",
    "for eachline in data:\n",
    "    oneobs = eachline.split('\\t')\n",
    "    if len(oneobs) == 11:\n",
    "        danmucode.append(oneobs[1])\n",
    "        danmulist.append(oneobs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77971\n",
      "77971\n"
     ]
    }
   ],
   "source": [
    "print(len(danmucode))\n",
    "print(len(danmulist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######read reply1:\n",
    "# rurl,page,totalPage,allCount,totalCount,cid,quoteId,content,postDate,userID,\n",
    "# userName,coun_t,deep,refCount,ups,downs,nameRed,avatarFrame,isDelete,isUpDelete,\n",
    "# verified,crawl_time\n",
    "# reply2:\n",
    "# line = [rurl,int(video_code),page,totalPage,allCount,totalCount,cid,quoteId,content,postDate,userID,userName,coun_t,deep,refCount,ups,downs,nameRed,avatarFrame,isDelete,isUpDelete,verified,crawl_time]\n",
    "import re\n",
    "with open(\"allacfunreply_1.txt\", encoding = \"utf-8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "replylist = []\n",
    "replycode = []\n",
    "for eachline in data:\n",
    "    oneobs = eachline.split('\\t')\n",
    "    if len(oneobs) == 23:\n",
    "        excode = re.findall(r'contentId=(\\d+)&current',oneobs[0])[0]\n",
    "        replycode.append(excode)\n",
    "        replylist.append(oneobs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[emot=ac,01/]up主被封禁!',\n",
       " '[emot=ac,43/]远古巨坟～坟中坟～',\n",
       " 'UP，你还在a站吗[emot=ac2,55/]',\n",
       " '听说这边有远古坟，慕名而来٩(*´◒`*)۶',\n",
       " '我想知道七年后有没有人来挖我',\n",
       " '一个考古员那，来到acfun那，每天都会挖到一个优秀的古坟！！！[emot=ac3,48/]',\n",
       " '终极蛇皮坟',\n",
       " '大家挖的好快啊！[emot=ac,19/]',\n",
       " '祖坟。。。。。',\n",
       " '超级古坟  留个名']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(replycode))\n",
    "print(len(replylist))\n",
    "replylist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def posneg(docu):\n",
    "    try:\n",
    "        point = posneg_score(sentiment_score_list(docu))\n",
    "        return point\n",
    "    except:\n",
    "        point = 0\n",
    "        return point\n",
    "vc_posneg = np.vectorize(posneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 209.667556 s\n"
     ]
    }
   ],
   "source": [
    "# rate the danmu\n",
    "import time\n",
    "start = time.clock()\n",
    "#long running\n",
    "sentilist = vc_posneg(danmulist)\n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 189s to run 77971 lines of danmu (12.6Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77971\n",
      "77971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#videocode, danmulist, sentitlist\n",
    "print(len(danmulist))\n",
    "print(len(sentilist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>danmu</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66</td>\n",
       "      <td>你们只是一群伴舞的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66</td>\n",
       "      <td>tdgsgl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>睡觉</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>这坟太远古  我不敢看</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>‎2016‎年‎4‎月‎26‎日17:02:23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66</td>\n",
       "      <td>吃我一击洛阳铲</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66</td>\n",
       "      <td>就只有66。。远古巨坟</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                     danmu  senti\n",
       "0   66                 你们只是一群伴舞的      0\n",
       "1   66                    tdgsgl      0\n",
       "2   66                        睡觉      0\n",
       "3   66               这坟太远古  我不敢看      1\n",
       "4   66  ‎2016‎年‎4‎月‎26‎日17:02:23      0\n",
       "5   66                   吃我一击洛阳铲      0\n",
       "6   66               就只有66。。远古巨坟      0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a danmu_sent dataframe\n",
    "# lst1 = range(100)\n",
    "# lst2 = range(100)\n",
    "# lst3 = range(100)\n",
    "# percentile_list = pd.DataFrame(\n",
    "#     {'lst1Tite': lst1,\n",
    "#      'lst2Tite': lst2,\n",
    "#      'lst3Tite': lst3\n",
    "#     })\n",
    "import pandas as pd\n",
    "#danmucode, danmulist, sentilist\n",
    "dansendf = pd.DataFrame(\n",
    "    {'code':danmucode,\n",
    "     'danmu':danmulist,\n",
    "     'senti':sentilist}\n",
    ")\n",
    "dansendf.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posneg_score(sentiment_score_list( '一个考古员那，来到acfun那，每天都会挖到一个优秀的古坟！！！[emot=ac3,48/]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 258.696734 s\n"
     ]
    }
   ],
   "source": [
    "# rate the reply\n",
    "import time\n",
    "start = time.clock()\n",
    "#long running\n",
    "re_sentilist = []\n",
    "for rep in replylist:\n",
    "    re_sentilist.append(posneg(rep))\n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 258s to run 32000 lines of reply (12.4Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>reply</th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1171</td>\n",
       "      <td>[emot=ac,01/]up主被封禁!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1171</td>\n",
       "      <td>[emot=ac,43/]远古巨坟～坟中坟～</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>UP，你还在a站吗[emot=ac2,55/]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>听说这边有远古坟，慕名而来٩(*´◒`*)۶</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>我想知道七年后有没有人来挖我</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66</td>\n",
       "      <td>一个考古员那，来到acfun那，每天都会挖到一个优秀的古坟！！！[emot=ac3,48/]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66</td>\n",
       "      <td>终极蛇皮坟</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>66</td>\n",
       "      <td>大家挖的好快啊！[emot=ac,19/]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>66</td>\n",
       "      <td>祖坟。。。。。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>66</td>\n",
       "      <td>超级古坟  留个名</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                                           reply  senti\n",
       "0  1171                            [emot=ac,01/]up主被封禁!      0\n",
       "1  1171                          [emot=ac,43/]远古巨坟～坟中坟～      0\n",
       "2   114                         UP，你还在a站吗[emot=ac2,55/]      0\n",
       "3    66                          听说这边有远古坟，慕名而来٩(*´◒`*)۶      0\n",
       "4    66                                  我想知道七年后有没有人来挖我      1\n",
       "5    66  一个考古员那，来到acfun那，每天都会挖到一个优秀的古坟！！！[emot=ac3,48/]      1\n",
       "6    66                                           终极蛇皮坟      0\n",
       "7    66                           大家挖的好快啊！[emot=ac,19/]      0\n",
       "8    66                                         祖坟。。。。。      0\n",
       "9    66                                       超级古坟  留个名      1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a reply_senti dataframe\n",
    "#replycode, replylist, re_sentilist\n",
    "repsendf = pd.DataFrame(\n",
    "    {'code':replycode,\n",
    "     'reply':replylist,\n",
    "     'senti':re_sentilist}\n",
    ")\n",
    "repsendf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'66'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the video\n",
    "with open(\"allacvideo_1.txt\", encoding = \"utf-8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "danmulist = []\n",
    "videocode = []\n",
    "for eachline in data:\n",
    "    oneobs = eachline.split('\\t')\n",
    "    if len(oneobs) == 18:\n",
    "        danmucode.append(oneobs[1])\n",
    "        videocode.append(oneobs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pic.Timeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题分析 (LDA model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "###########弹幕\n",
    "# 1. 导入\n",
    "import pandas as pd  \n",
    "import jieba  \n",
    "import nltk  \n",
    "import jieba.posseg as pseg  \n",
    "from gensim import corpora, models, similarities  \n",
    "#pip install nltk, jieba, gensim\n",
    "# warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dandf = pd.DataFrame(danmulist,columns=[\"danmu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danmu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>你们只是一群伴舞的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tdgsgl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>睡觉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>这坟太远古  我不敢看</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‎2016‎年‎4‎月‎26‎日17:02:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      danmu\n",
       "0                 你们只是一群伴舞的\n",
       "1                    tdgsgl\n",
       "2                        睡觉\n",
       "3               这坟太远古  我不敢看\n",
       "4  ‎2016‎年‎4‎月‎26‎日17:02:23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dandf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = dandf['danmu']\n",
    "type(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\l\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.002 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 82.098686 s\n"
     ]
    }
   ],
   "source": [
    "# 2. 分词+选词  \n",
    "import time\n",
    "start = time.clock()\n",
    "#long running\n",
    "nwordall = []  \n",
    "for t in cont:  \n",
    "    words =pseg.cut(t)  \n",
    "    nword = ['']  \n",
    "    for w in words:  \n",
    "#         if((w.flag == 'n'or w.flag == 'v' or w.flag == 'a') and len(w.word)>1):  \n",
    "        if(len(w.word)>1):  \n",
    "            nword.append(w.word)  \n",
    "    nwordall.append(nword) \n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 77s to run 77971 lines of danmu (12.6Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', '你们', '只是', '一群', '伴舞'],\n",
       " ['', 'tdgsgl'],\n",
       " ['', '睡觉'],\n",
       " ['', '远古', '不敢'],\n",
       " ['', '2016', '26', '17', '02', '23'],\n",
       " ['', '我一', '洛阳'],\n",
       " ['', '只有', '66', '远古'],\n",
       " ['', '在线', '对不起'],\n",
       " ['', '在线', '偶像', '大师'],\n",
       " ['', '爆音']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nwordall[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 1.262779 s\n"
     ]
    }
   ],
   "source": [
    "# 3. 选择后的词生成字典  \n",
    "start = time.clock()\n",
    "\n",
    "dictionary = corpora.Dictionary(nwordall)  \n",
    "# 生成语料库   \n",
    "corpus = [dictionary.doc2bow(text) for text in nwordall]  \n",
    "#tfidf加权  \n",
    "tfidf = models.TfidfModel(corpus)  \n",
    "corpus_tfidf = tfidf[corpus]  \n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 1s to run 77971 lines of danmu (12.6Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (dictionary.token2id)\n",
    "# print (tfidf) \n",
    "# for doc in corpus_tfidf:  \n",
    "#      print (doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, '0.048*\"color\" + 0.034*\"继续\" + 0.027*\"不会\" + 0.026*\"消失\" + 0.021*\"nbsp\" + 0.018*\"13\" + 0.015*\"TMD\" + 0.013*\"感谢\" + 0.012*\"NTR\" + 0.012*\"专业\"')\n",
      "(44, '0.032*\"下载\" + 0.021*\"ac\" + 0.020*\"作者\" + 0.019*\"emot\" + 0.016*\"07\" + 0.015*\"就是\" + 0.015*\"福利\" + 0.014*\"高清\" + 0.013*\"17\" + 0.013*\"养肥\"')\n",
      "(28, '0.082*\"..\" + 0.044*\"我们\" + 0.035*\"一直\" + 0.030*\"AC\" + 0.021*\"一发\" + 0.018*\"首页\" + 0.016*\"53\" + 0.015*\"有个\" + 0.014*\"而已\" + 0.012*\"正好\"')\n",
      "runtime: 22.999903 s\n"
     ]
    }
   ],
   "source": [
    "# 4. 主题模型lda，可用于降维  \n",
    "start = time.clock()\n",
    "\n",
    "#lda计算，每块20000条记录，提取50个主题  \n",
    "lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=50,     update_every=1, chunksize=10000, passes=1)  \n",
    "for i in range(0,3):  \n",
    "    print (lda.print_topics(i)[0])  \n",
    "#lda全部数据建模，提取100个主题  \n",
    "#lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=100, update_every=0, passes=20)  \n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 50s to run 77971 lines of danmu (12.6Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_lda = lda[corpus_tfidf]  \n",
    "# doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:不好,similar:0.9994120597839355 \n",
      "word:刚刚,similar:0.9992666840553284 \n",
      "word:肯定,similar:0.9991625547409058 \n",
      "word:不管,similar:0.9991371631622314 \n",
      "word:衣服,similar:0.9990770220756531 \n",
      "word:然后,similar:0.9990596175193787 \n",
      "word:孩子,similar:0.9990497827529907 \n",
      "word:看见,similar:0.9990304708480835 \n",
      "word:演员,similar:0.9989286065101624 \n",
      "word:里面,similar:0.9988921284675598 \n",
      "#################\n",
      "word:生活,similar:0.9994906187057495 \n",
      "word:100,similar:0.9994415044784546 \n",
      "word:小时,similar:0.9993937015533447 \n",
      "word:路过,similar:0.9993805885314941 \n",
      "word:PK,similar:0.9993515014648438 \n",
      "word:第二,similar:0.9993427991867065 \n",
      "word:系统,similar:0.9993183612823486 \n",
      "word:目测,similar:0.9993164539337158 \n",
      "word:ac,similar:0.9992858171463013 \n",
      "word:60,similar:0.9992822408676147 \n",
      "runtime: 1.489941 s\n"
     ]
    }
   ],
   "source": [
    "#5. word2vec 词向量化，可用于比较词相似度，寻找对应关系，词聚类  \n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "#sentences = models.word2vec.LineSentence(nwordall)  \n",
    "#size为词向量维度数,windows窗口范围,min_count频数小于5的词忽略,workers是线程数  \n",
    "model = models.word2vec.Word2Vec(nwordall, size=100, window=5, min_count=5, workers=4)  \n",
    "model.save(\"F:\\word2vecmodels\") #如果建模速度慢的话，建议保存，后续直接调用  \n",
    "model = models.word2vec.Word2Vec.load(\"F:\\word2vecmodels\")  \n",
    "# print (model[u'童年'])  \n",
    "#向量表示  \n",
    "# sim = model.most_similar(positive=[u'童年', u'传说'])  \n",
    "sim = model.most_similar(positive=[u'童年'])  \n",
    "\n",
    "#相近词  \n",
    "for s in sim:  \n",
    "    print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "print(\"#################\")\n",
    "sim = model.most_similar(positive=[u'传说']) \n",
    "for s in sim:  \n",
    "    print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "#找到“童年”和”传说“相似度高的词  \n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim = model.most_similar(positive=[u'童年'])  \n",
    "# for s in sim:  \n",
    "#     print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "# sim = model.most_similar(positive=[u'传说'])  \n",
    "# for s in sim:  \n",
    "#     print (\"word:%s,similar:%s \" %(s[0],s[1])) \n",
    "# sim = model.most_similar(positive=[u'机智'])  \n",
    "# for s in sim:  \n",
    "#     print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "# sim = model.most_similar(positive=[u'QAQ'])  \n",
    "# for s in sim:  \n",
    "#     print (\"word:%s,similar:%s \" %(s[0],s[1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[emot=ac,01/]up主被封禁!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[emot=ac,43/]远古巨坟～坟中坟～</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UP，你还在a站吗[emot=ac2,55/]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>听说这边有远古坟，慕名而来٩(*´◒`*)۶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>我想知道七年后有没有人来挖我</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     reply\n",
       "0     [emot=ac,01/]up主被封禁!\n",
       "1   [emot=ac,43/]远古巨坟～坟中坟～\n",
       "2  UP，你还在a站吗[emot=ac2,55/]\n",
       "3   听说这边有远古坟，慕名而来٩(*´◒`*)۶\n",
       "4           我想知道七年后有没有人来挖我"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###########评论\n",
    "import pandas as pd  \n",
    "import jieba  \n",
    "import nltk  \n",
    "import jieba.posseg as pseg  \n",
    "from gensim import corpora, models, similarities  \n",
    "repdf = pd.DataFrame(replylist,columns=[\"reply\"])\n",
    "repdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = repdf['reply']\n",
    "type(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 107.742172 s\n"
     ]
    }
   ],
   "source": [
    "# 2. 分词+选词  \n",
    "import time\n",
    "start = time.clock()\n",
    "#long running\n",
    "nwordall = []  \n",
    "for t in cont:  \n",
    "    words =pseg.cut(t)  \n",
    "    nword = ['']  \n",
    "    for w in words:  \n",
    "#         if((w.flag == 'n'or w.flag == 'v' or w.flag == 'a') and len(w.word)>1):  \n",
    "        if(len(w.word)>1):  \n",
    "            nword.append(w.word)  \n",
    "    nwordall.append(nword) \n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 107s to run 32000 lines of reply (12.4Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0.625352 s\n"
     ]
    }
   ],
   "source": [
    "# 3. 选择后的词生成字典  \n",
    "start = time.clock()\n",
    "\n",
    "dictionary = corpora.Dictionary(nwordall)  \n",
    "# 生成语料库   \n",
    "corpus = [dictionary.doc2bow(text) for text in nwordall]  \n",
    "#tfidf加权  \n",
    "tfidf = models.TfidfModel(corpus)  \n",
    "corpus_tfidf = tfidf[corpus]  \n",
    "\n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 1s to run 32000 lines of reply (12.4Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, '0.055*\"压力\" + 0.033*\"解释\" + 0.021*\"很大\" + 0.018*\"有没有\" + 0.018*\"一般\" + 0.014*\"这里\" + 0.013*\"片子\" + 0.012*\"洗脑\" + 0.012*\"效果\" + 0.010*\"战斗\"')\n",
      "(31, '0.115*\"BGM\" + 0.043*\"来看\" + 0.034*\"碉堡\" + 0.024*\"百合\" + 0.024*\"记得\" + 0.017*\"2015\" + 0.017*\"07\" + 0.016*\"电视\" + 0.015*\"接受\" + 0.013*\"ED\"')\n",
      "(4, '0.082*\"红名\" + 0.057*\"可以\" + 0.054*\"最后\" + 0.033*\"OP\" + 0.031*\"哪里\" + 0.031*\"15\" + 0.027*\"ais\" + 0.026*\"23\" + 0.025*\"18\" + 0.024*\"试试\"')\n",
      "runtime: 26.338470 s\n"
     ]
    }
   ],
   "source": [
    "# 4. 主题模型lda，可用于降维  \n",
    "start = time.clock()\n",
    "\n",
    "#lda计算，每块10000条记录，提取50个主题  \n",
    "lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=50,     update_every=1, chunksize=1000, passes=1)  \n",
    "for i in range(0,3):  \n",
    "    print (lda.print_topics(i)[0])  \n",
    "#lda全部数据建模，提取100个主题  \n",
    "#lda = models.ldamodel.LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=100, update_every=0, passes=20)  \n",
    "end = time.clock()\n",
    "print(\"runtime: %f s\" % (end - start))\n",
    "#would take 23s to run 32000 lines of reply (12.4Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################与童年主题相近的词汇\n",
      "word:春晚,similar:0.9987530708312988 \n",
      "word:CG,similar:0.9984016418457031 \n",
      "word:除了,similar:0.9983508586883545 \n",
      "word:国产,similar:0.998162031173706 \n",
      "word:某些,similar:0.9980917572975159 \n",
      "word:甚至,similar:0.9980370998382568 \n",
      "word:上海,similar:0.9979149103164673 \n",
      "word:看起来,similar:0.9977549314498901 \n",
      "word:法师,similar:0.997751772403717 \n",
      "word:半年,similar:0.997745931148529 \n",
      "#################与传说相似的词汇\n",
      "word:听说,similar:0.9863263964653015 \n",
      "word:尼玛,similar:0.9858675003051758 \n",
      "word:泥煤,similar:0.9817530512809753 \n",
      "word:真的假,similar:0.981099009513855 \n",
      "word:真是,similar:0.9806280136108398 \n",
      "word:同一个,similar:0.9799231886863708 \n",
      "word:老子,similar:0.9797074198722839 \n",
      "word:难道,similar:0.9774042367935181 \n",
      "word:干事,similar:0.9749318957328796 \n",
      "word:红色,similar:0.9746084213256836 \n"
     ]
    }
   ],
   "source": [
    "#5. word2vec 词向量化，可用于比较词相似度，寻找对应关系，词聚类  \n",
    "\n",
    "#sentences = models.word2vec.LineSentence(nwordall)  \n",
    "#size为词向量维度数,windows窗口范围,min_count频数小于5的词忽略,workers是线程数  \n",
    "model = models.word2vec.Word2Vec(nwordall, size=100, window=5, min_count=5, workers=4)  \n",
    "model.save(\"F:\\word2vecmodels\") #如果建模速度慢的话，建议保存，后续直接调用  \n",
    "model = models.word2vec.Word2Vec.load(\"F:\\word2vecmodels\")  \n",
    "# print (model[u'童年'])  \n",
    "#向量表示  \n",
    "# sim = model.most_similar(positive=[u'童年', u'传说'])  \n",
    "sim = model.most_similar(positive=[u'童年'])  \n",
    "\n",
    "#相近词  \n",
    "print(\"#################与童年主题相近的词汇\")\n",
    "\n",
    "for s in sim:  \n",
    "    print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "sim = model.most_similar(positive=[u'传说']) \n",
    "print(\"#################与传说相似的词汇\")\n",
    "\n",
    "for s in sim:  \n",
    "    print (\"word:%s,similar:%s \" %(s[0],s[1]))  \n",
    "#找到“童年”和”传说“相似度高的词  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
